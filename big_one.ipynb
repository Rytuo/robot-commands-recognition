{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install git+https://github.com/huggingface/transformers -qqq\n",
        "!pip install sentencepiece -qqq\n",
        "!pip install bitsandbytes -qqq\n",
        "!pip install accelerate -qqq\n",
        "!pip uninstall nvidia_cublas_cu11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X27jWaAWr7bf",
        "outputId": "205a7691-dbdc-41ad-a4c9-205a1955ff96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping nvidia_cublas_cu11 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "ReFAWqhMsE8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!g1.1\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class JsonDataset(Dataset):\n",
        "    def __init__(self, filename, img_dir, transform=None, verbose=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.data = []\n",
        "        self.transform = transform\n",
        "        self.img_dir = img_dir\n",
        "        # self.blank_image = Image.open(os.listdir(self.img_dir)[0])\n",
        "\n",
        "        with open(filename) as f:\n",
        "            # load the JSON file\n",
        "            json_data = json.load(f)\n",
        "            # process the loaded JSON file\n",
        "            for item in json_data:\n",
        "                promt = item['goal_eng']\n",
        "                image_path = os.path.join(img_dir, item['image'])\n",
        "\n",
        "                plan = item['plan']\n",
        "\n",
        "                self.data.append((promt, image_path, plan))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        promt = self.data[index][0]\n",
        "        image_path = self.data[index][1]\n",
        "        image = Image.open(image_path)\n",
        "        plan = self.data[index][2]\n",
        "        return promt, image, plan"
      ],
      "metadata": {
        "id": "pxovPEl2sH0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!g1.1\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Define the encoder class\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=31, truncation_side='left')\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.embedding_size = self.model.config.hidden_size\n",
        "\n",
        "        # Freeze the weights of the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, text_sequences):\n",
        "        inputs = self.tokenizer(text_sequences, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # the sequence of hidden-states at the output of the last layer\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        return last_hidden_states\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        self.embedding_size = self.model.config.projection_dim\n",
        "\n",
        "        # Freeze the weights of the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, img: Image):\n",
        "        inputs = self.processor(\n",
        "            text=[\"What robot sees\"],\n",
        "            images=img,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        )\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.image_embeds\n",
        "\n",
        "class GeneralEncoder(nn.Module):\n",
        "    def __init__(self, encoder_out_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder_out_dim = encoder_out_dim\n",
        "\n",
        "        self.text_encoder = TextEncoder(\"bert-base-uncased\") # freezed\n",
        "        self.image_encoder = ImageEncoder() # freezed\n",
        "        self.text_linear = nn.Linear(self.text_encoder.embedding_size, self.encoder_out_dim)\n",
        "        self.image_linear = nn.Linear(self.image_encoder.embedding_size, self.encoder_out_dim)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_features = self.text_encoder(text)\n",
        "        image_features = self.image_encoder(image)\n",
        "\n",
        "        text_output = self.text_linear(text_features)\n",
        "        image_output = self.image_linear(image_features)\n",
        "\n",
        "        output = torch.cat((image_output.unsqueeze(1), text_output), dim=1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "rzTritsZsUQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ler7RiFLSzuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzXOOoIxrr9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zj-Wn-MDRM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}